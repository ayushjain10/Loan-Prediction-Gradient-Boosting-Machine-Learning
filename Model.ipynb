{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "#System level library\n",
    "import sys\n",
    "\n",
    "#Library for scientific computation\n",
    "import scipy\n",
    "\n",
    "#Library for graph plotting (For visualization)\n",
    "import matplotlib\n",
    "\n",
    "#Library for analyzing tabular data\n",
    "import pandas\n",
    "\n",
    "#Library to perform vector/array operation\n",
    "import numpy\n",
    "\n",
    "#Library for machine learning algorithm\n",
    "import sklearn\n",
    "\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainurl = '/Users/ayushjain/Downloads/Loan Prediction Project/clean data/cleaned_train_data.csv'\n",
    "testurl = '/Users/ayushjain/Downloads/Loan Prediction Project/clean data/cleaned_test_data.csv'\n",
    "\n",
    "train = pandas.read_csv(trainurl)\n",
    "test = pandas.read_csv(testurl)\n",
    "\n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#Convert all the non-numeric data to numeric data by using dummies function from pandas library\n",
    "cols = ['Gender','Married','Dependents','Education','Self_Employed','Credit_History','Property_Area']\n",
    "\n",
    "traindata = pandas.get_dummies(train, columns = cols)\n",
    "testdata = pandas.get_dummies(test, columns = cols)\n",
    "\n",
    "Y = traindata.Loan_Status\n",
    "\n",
    "loanid = test.Loan_ID\n",
    "\n",
    "traindata = traindata.drop(['Loan_ID', 'Loan_Status'], axis = 1)\n",
    "\n",
    "X = traindata.astype(numpy.float32).reset_index(drop=True)\n",
    "\n",
    "testdata = testdata.drop('Loan_ID', axis = 1)\n",
    "\n",
    "testdata.insert(8, 'Married_Unknown', 0)\n",
    "\n",
    "#Using the train_test_split() to create both training and test data.\n",
    "#Training data is used to create the model\n",
    "#Test data is used for validation of the model which was created using training data\n",
    "\n",
    "#Let's split the full_data in 80-20, where 80% is training data & test_data as 20%\n",
    "test_data_size = 0.20\n",
    "\n",
    "train_test_list = model_selection.train_test_split(X, Y, test_size = test_data_size, random_state = 10)\n",
    "\n",
    "X_train_data = train_test_list[0]\n",
    "\n",
    "X_test_data = train_test_list[1]\n",
    "\n",
    "Y_train_data = train_test_list[2]\n",
    "\n",
    "Y_test_data = train_test_list[3]\n",
    "\n",
    "#We have splitted the data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning rate:0.05\n",
      "Estimators:20\n",
      "Score:0.7107942973523421\n",
      "\n",
      "For learning rate:0.25\n",
      "Estimators:20\n",
      "Score:0.8167006109979633\n",
      "\n",
      "For learning rate:0.5\n",
      "Estimators:20\n",
      "Score:0.8187372708757638\n",
      "\n",
      "For learning rate:0.75\n",
      "Estimators:20\n",
      "Score:0.8289205702647657\n",
      "\n",
      "For learning rate:1\n",
      "Estimators:20\n",
      "Score:0.8289205702647657\n",
      "\n",
      "For learning rate:0.05\n",
      "Estimators:50\n",
      "Score:0.8065173116089613\n",
      "\n",
      "For learning rate:0.25\n",
      "Estimators:50\n",
      "Score:0.8268839103869654\n",
      "\n",
      "For learning rate:0.5\n",
      "Estimators:50\n",
      "Score:0.8492871690427699\n",
      "\n",
      "For learning rate:0.75\n",
      "Estimators:50\n",
      "Score:0.8615071283095723\n",
      "\n",
      "For learning rate:1\n",
      "Estimators:50\n",
      "Score:0.8757637474541752\n",
      "\n",
      "For learning rate:0.05\n",
      "Estimators:100\n",
      "Score:0.8167006109979633\n",
      "\n",
      "For learning rate:0.25\n",
      "Estimators:100\n",
      "Score:0.835030549898167\n",
      "\n",
      "For learning rate:0.5\n",
      "Estimators:100\n",
      "Score:0.879837067209776\n",
      "\n",
      "For learning rate:0.75\n",
      "Estimators:100\n",
      "Score:0.8940936863543788\n",
      "\n",
      "For learning rate:1\n",
      "Estimators:100\n",
      "Score:0.9103869653767821\n",
      "\n",
      "For learning rate:0.05\n",
      "Estimators:200\n",
      "Score:0.8187372708757638\n",
      "\n",
      "For learning rate:0.25\n",
      "Estimators:200\n",
      "Score:0.8778004073319755\n",
      "\n",
      "For learning rate:0.5\n",
      "Estimators:200\n",
      "Score:0.9205702647657841\n",
      "\n",
      "For learning rate:0.75\n",
      "Estimators:200\n",
      "Score:0.945010183299389\n",
      "\n",
      "For learning rate:1\n",
      "Estimators:200\n",
      "Score:0.9592668024439919\n",
      "\n",
      "For learning rate:0.05\n",
      "Estimators:300\n",
      "Score:0.8207739307535642\n",
      "\n",
      "For learning rate:0.25\n",
      "Estimators:300\n",
      "Score:0.8981670061099797\n",
      "\n",
      "For learning rate:0.5\n",
      "Estimators:300\n",
      "Score:0.9531568228105907\n",
      "\n",
      "For learning rate:0.75\n",
      "Estimators:300\n",
      "Score:0.9653767820773931\n",
      "\n",
      "For learning rate:1\n",
      "Estimators:300\n",
      "Score:0.9877800407331976\n",
      "\n",
      "For learning rate:0.05\n",
      "Estimators:400\n",
      "Score:0.8289205702647657\n",
      "\n",
      "For learning rate:0.25\n",
      "Estimators:400\n",
      "Score:0.9103869653767821\n",
      "\n",
      "For learning rate:0.5\n",
      "Estimators:400\n",
      "Score:0.9714867617107943\n",
      "\n",
      "For learning rate:0.75\n",
      "Estimators:400\n",
      "Score:0.9959266802443992\n",
      "\n",
      "For learning rate:1\n",
      "Estimators:400\n",
      "Score:0.9959266802443992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We will now run the Gradient Boosting Algorithm on the training data and fit the test data. \n",
    "\n",
    "#For the one with which we get highest accuracy, we will take that parameters and fit the model using that parameters.\n",
    "\n",
    "#Then we estimate the Loan_Status on our test data set.\n",
    "\n",
    "#Learning rate : It determines the impact of each tree on the final outcome.\n",
    "#number of estimators : Number of sequential trees to be modeled.\n",
    "#The more the number of estimators, the better. But beware, we may overfit the model from training data.\n",
    "#Choosing the value for n_estimator is critical.\n",
    "learning_rates = [0.05, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "n_estimators = [20,50,100,200,300,400]\n",
    "\n",
    "for n_estimator in n_estimators:\n",
    "    for learning_rate in learning_rates:\n",
    "        gb = GradientBoostingClassifier(n_estimators = n_estimator, learning_rate = learning_rate, max_features = 2, max_depth = 2)\n",
    "        gb.fit(X_train_data, Y_train_data)\n",
    "        print('For learning rate:'+str(learning_rate))\n",
    "        print('Estimators:'+ str(n_estimator))\n",
    "        print('Score:'+str(gb.score(X_train_data, Y_train_data)))\n",
    "        print()\n",
    "        \n",
    "#We see that with 400 estimators we get almost 100% accuracy, but by choosing 400 estimators we are overfitting training dataset.\n",
    "\n",
    "#Choosing the parameters is very CRITICAL.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.75, loss='deviance', max_depth=2,\n",
       "              max_features=2, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's choose number of estimators as 300 and learning rate as 0.75\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators = 300, learning_rate = 0.75, max_features = 2, max_depth = 2)\n",
    "\n",
    "#Fit the model on our training dataset\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001015</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001022</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001031</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001035</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001051</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LP001054</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LP001055</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LP001056</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LP001059</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LP001067</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LP001078</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LP001082</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LP001083</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LP001094</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LP001096</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LP001099</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LP001105</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LP001107</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LP001108</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LP001115</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LP001121</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LP001124</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LP001128</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LP001135</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LP001149</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LP001153</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LP001163</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LP001169</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LP001174</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LP001176</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>LP002856</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>LP002857</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>LP002858</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>LP002860</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>LP002867</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>LP002869</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>LP002870</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>LP002876</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>LP002878</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>LP002879</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>LP002885</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>LP002890</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>LP002891</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>LP002899</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>LP002901</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>LP002907</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>LP002920</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>LP002921</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>LP002932</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>LP002935</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>LP002952</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>LP002954</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>LP002962</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>LP002965</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>LP002969</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>LP002971</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>LP002975</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>LP002980</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>LP002986</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>LP002989</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Loan_ID  0\n",
       "0    LP001015  Y\n",
       "1    LP001022  Y\n",
       "2    LP001031  Y\n",
       "3    LP001035  Y\n",
       "4    LP001051  N\n",
       "5    LP001054  Y\n",
       "6    LP001055  Y\n",
       "7    LP001056  N\n",
       "8    LP001059  Y\n",
       "9    LP001067  Y\n",
       "10   LP001078  Y\n",
       "11   LP001082  Y\n",
       "12   LP001083  Y\n",
       "13   LP001094  Y\n",
       "14   LP001096  Y\n",
       "15   LP001099  Y\n",
       "16   LP001105  Y\n",
       "17   LP001107  Y\n",
       "18   LP001108  Y\n",
       "19   LP001115  Y\n",
       "20   LP001121  Y\n",
       "21   LP001124  N\n",
       "22   LP001128  N\n",
       "23   LP001135  Y\n",
       "24   LP001149  Y\n",
       "25   LP001153  N\n",
       "26   LP001163  Y\n",
       "27   LP001169  Y\n",
       "28   LP001174  Y\n",
       "29   LP001176  Y\n",
       "..        ... ..\n",
       "337  LP002856  Y\n",
       "338  LP002857  N\n",
       "339  LP002858  N\n",
       "340  LP002860  Y\n",
       "341  LP002867  Y\n",
       "342  LP002869  N\n",
       "343  LP002870  N\n",
       "344  LP002876  Y\n",
       "345  LP002878  Y\n",
       "346  LP002879  N\n",
       "347  LP002885  N\n",
       "348  LP002890  Y\n",
       "349  LP002891  N\n",
       "350  LP002899  Y\n",
       "351  LP002901  Y\n",
       "352  LP002907  Y\n",
       "353  LP002920  Y\n",
       "354  LP002921  N\n",
       "355  LP002932  Y\n",
       "356  LP002935  Y\n",
       "357  LP002952  Y\n",
       "358  LP002954  Y\n",
       "359  LP002962  Y\n",
       "360  LP002965  Y\n",
       "361  LP002969  Y\n",
       "362  LP002971  Y\n",
       "363  LP002975  Y\n",
       "364  LP002980  Y\n",
       "365  LP002986  N\n",
       "366  LP002989  Y\n",
       "\n",
       "[367 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have our model ready.\n",
    "#Let's make predictions on the test dataset\n",
    "\n",
    "predictions = model.predict(testdata)\n",
    "\n",
    "predictions = pandas.Series(predictions)\n",
    "\n",
    "final = pandas.concat([loanid, predictions], names = ['Loan_ID', 'Loan_Status'], axis = 1)\n",
    "\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have successfully imputed the values on the test dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
